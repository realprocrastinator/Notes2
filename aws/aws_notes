
#======================================================================================
# IAM
-Overview User: Individual end user Groups: Collection of users under one set of permissions Role: that has certain access permission that can be assigned to client or AWS resources
Policies: A document that defines one or more permissions, that can be attached to a role
          ,client, group or resources.

-Acess(Console, cmd)
when attached a user to a group then group attach to a policy
the secrete access key and the access key will be used when command line log into aws
username and password are used to log into the console

-Roles:
AM roles are a secure way to grant permissions to entities that you trust. 
Examples of entities include the following:

>IAM user in another account
>Application code running on an EC2 instance that needs to perform actions on AWS resources
>An AWS service that needs to act on resources in your account to provide its features
>Users from a corporate directory who use identity federation with SAML
>IAM roles issue keys that are valid for short durations, making them a more secure way to grant access.

-Summary:
>IAM is gloabal , which would apply to different regions
>Root Account is the account that when first create the AWS account, which has root acess
>Always set up multi factor authentication on the root account
>Create your own password policies

#======================================================================================
# EC2
#-------------------------------------------------------------------------------
4 ways to use EC2
On Demand 
  pay a fix rate per hour (or second) with no commitment
Reserved 
  get a discount for reserving ahead of time (and paying upfront), based on expected usage 
  (1 Year or 3 Year terms)
Spot 
  if youâ€™re flexible, bid when the price is right
Dedicated hosts 
  dedicated physical EC2 servers (allowing you to use your server-bound licenses, like VMWare of Oracle)

#-------------------------------------------------------------------------------
EC2 Instance type
F for FPGA,Field Programmable Gate Array Genomic:Genomics research, financial Analysis, real time
video processing, big data 
I for IOPS, High Speed Storage: Data Warehouse
G - Graphics:Video Encoding/3D Application Streaming
H - High Disk Throughput: MapReduced based workloads, DFS(HDFS)
T - cheap general purpose(T2 Micro)
D - Density: Datawarehouse,Hadoop
R - Ram: Memory Intensive Apps/DB
M - main choice for general purpose apps
C - Compute,CPU intensive Apps/DB
P - Graphics(Pics), Machine learning or Bitcoin mininig
X - Extreme Memory,SAP HANA/Apache Spark etc

#-------------------------------------------------------------------------------
EBS Types
-General Purpose:usualy less thant 10,000 IPOS
-Provisional IOPS SSD(IO1):I/O intensive work like databse( more than 10,000 IOPS )
-Throughput Optimised HDD(ST1):Big data, Datawarehouse(can not be a boot volume)
-Cold HDD (SC1): lowest cost storage for infrequenctly accessed workloads(can not be a boot volume)
-Magnetic(Standard): Lowest cost per gigabyte of all EBS volume types that is bootable
  
#-------------------------------------------------------------------------------
Load Balancer
-Application Load Balancer:load balance according to application needs,
    can be application request dependant.
-Network Load Balancer:Balance at network layer with fast speed   
-Classic Load Balancer:can load balance http/https applications and use layer
7-specific features. You can also use strict layer 4 load balancing for applications
that reply purely on the TCP protocol.

504 gateway time out error , application did not reply within the timeout need to check
the error in the application

Since the server would only see the ip address of the load balancer, if it wants to
see the source IP of the client, it can check the Forwarded-For header
#-------------------------------------------------------------------------------
#https://manage.vodien.com.au/ #DNS management
# install Apache in the EC2 instance
yum update -y # get the distribution update
yum install httpd -y # install apache server
service httpd start # start  the apache server
chkconfig httpd on # automatically start server when reboot
service httpd status # status
# html should be saved in the /var/www/html for server to find it 

#-------------------------------------------------------------------------------
Route53
Amazon's DNS service allow to map your domain name to
EC2 Instance
Load Balancer
S3 Buckets


#-------------------------------------------------------------------------------
# CLI
# https://docs.aws.amazon.com/cli/latest/index.html
# the region if left default would be the region that this instance is created 
# ~/.aws/config ~/.aws/credentials will be automatically generated if first use
# aws configure , and for later use based on first log in input
aws configure # using access ID and acess secerete key to log in
aws s3 ls
aws s3 mb s3://mys3bucket # make a s3 bucket
aws s3 cp hello.txt s3://mys3bucket # copy a local file to the s3 bucket
aws s3 ls s3://mys3bucket # will list the current file in this bucket 

-summary:
>Least privillege: should give user the least amount of access
>Create Groups: Users that added in the group will inherited the access previllege of
  group, group permission are assigned by the policies.
> Secrete Access Key: You will only see it once when you generate it,need to be saved in
  a safe place, or can delte the original one and regenerate a new key and run aws configure
> Do not share once access key: each person will have a unique access key

#-------------------------------------------------------------------------------
# S3 Role
> Create a Role and then goes to EC2 instance action -> instance setting-> assign role
# if previously used aws configure and used the access id and access secrete
# need to delte the saved credential saved in ~/.aws/credentials as well as ~/.aws/config

-summary:
> Role allows not to use Access Key ID and Secrete Access Keys
> Role are preferred from a security perspective(because secrete could be comprimised,
need to save the secrete), role will not be compromised, since it is from the server side
> Can cheange a policy on a role and it will take immediate effect.
> You can attach and detach roles to running EC2 instances without having to stop or terminate
these instances

#======================================================================================
# RDB

-OLTP vs OLAP:
OLTP(Online Transaction Processing):
  processing line transaction data , like order ,Name , date ,Deliery address
OLAP(Online Analitical Processing):
  some processing on the transaction raw data, like sales price - unit cost

-Elasticache
web service that is easy to deploy, operate and scale in memory cahce in the cloud.
Could improve the performance of the web by allowing to get access to cahce instead of
reading from database disk e.g. the 10 most frequently accessed data items
  
Open Source in memory cache engines:
Memcahced
Reis

-summary
> RDS:SQL,MySQL,Aurora,...
>No SQL:DynamoDB
> OLAP: RedShift, for BI big data processing
> Elasticache: In memory Caching

#-------------------------------------------------------------------------------
#RDB lab
> EC2 instance with the boot script to start the apache server and the mysql php
#!/bin/bash  
yum install httpd php php-mysql -y  
yum update -y  
chkconfig httpd on  
service httpd start  
echo "<?php phpinfo();?>" > /var/www/html/index.php
cd /var/www/html  
wget https://s3.amazonaws.com/acloudguru-production/connect.php
> create an MySQL DB with the security group to allow the the Ec2 access

#-------------------------------------------------------------------------------
# Non Relational Datbase
> Collection = Table
> Document = Row
> Key Value Pairs = Field

# ==================================================================
# RDS Mutiple A-Z Read Replicas and back up
# Automated Backups
> Automated backup can recover data to any points within a retention period(one or 35 days).
It will take a full daily snapshot and will store transaction logs throughout the day.
When recover aws would choose the most recent daily backup and then apply the transaction logs
relevant to that day.
> Automated Backups are enabled by default. The backup data is stored in S3 and will get free
storage space equal to the size of the the database1kjkj
> Backups are taken within a defined window, during the backup window , the storage IO may be 
suspeneded while your data is being backed up and might experience elevated latency.

# snapshot
> user initiated, peresistent even after the original RDS was deleted

# Restoring Backups
> when restoring a backup or snapshot, the database will be a new RDS instance with a new 
DNS endpoint

# Encyrption
> Encyrption is done using the Aws Key Management Service
if the original databse is encrypted ,same as the back up snapshot
and its replicas.
> Encrypting an existing DB instance is not supported,have to first
create a snapshot, make a copy of the snapshot and encrypt the copy.

# Multi-AZ RDS(synchrous)
> A copy of your production database in another availability zone. write to the primary DB
will automatically updated to the copy.
> In the event of primary DB failure, Amazon RDS will automatically failover to the standby
> It is only for disaster recover , not for performance, if need performance improvement, 
need Read Replicas.

# Read Replica
> read only copy of the primary database.Achieved by asynchronous replication from the
primary RDS instance to the read replica. Use for read heavy databse workload.
> available for MySQL Sever, PostgreSQL, MariaDB, Aurora.
> used for scaling ,not for data recovering.
> must have a automatic backups turned on in order to deploy a read replica.
> can have up to 5 read replica copies of any database.
> can have read replica of read replica.
> each read replica will have its own DNS endpoint.
> read replicas can have Multi-AZ.
> can create read replicas of Multi-AZ source database.
> read replicas can be promoted to be their own database. This would break the replication.
> read replica can be in a different region.


# ==================================================================
# ElasticCache
> improves the performance of the web by allowing to retrive information from fast , managed,
in memory caches, instead of relying entirely on the slow disk IO.

# Memcached:
> A widely adopted memory object caching system. ElasticCache is protocol compilant with 
memcahced.Popular tools with existing Memcahced environment is also compatible with the service.
> pure caching solution with no persistence, ElasticCache manages Memchaced nodes as a pool
that can grow and shrink, similar to an Amazon EC2 Auto Scaling Group. Individual nodes are
expendable, and ElastiCache provides additional capabilities here, such as automatic node
replacement and Auto Discovery.
# use case
> object caching is the primary goal, to offload the database.
> simple cahcing model.
> runing large cache nodes and require multithread performance with untilization of mutiple cores. 
> scale cache horizontally as you grow.

# Redis:
> open source key value store that support data structure like sorted sets, and lists.
ElasticCache supports Master/Slave replication and Multi-AZ which can be used to
achieve cross AZ redundancy.
> because of the replication and peresistent features of Redis, ElasticCache manages Redis
More as a relational database. Redis ElasticCache clusters are mananaged as stateful entities
taht include failover, similar to how Amazon RDS manages database failover.
# use case
> more advanced data types like lists, hashes and sets.
> sorting and ranking dataset in memory like leaderboards.
> want to run in multiple AWS Availability Zone.

-summary
> ElasticCache is good if DB is read-heavy and not prone to frequent changing
> if just to take of the load of the DB can simply use Memcahced
> if do some analysis then choose Redshift. 
# use Memcahced
> object caching is the primary goal
> keep things as simple as possible
> scale cache horizontally
# use Redis
> hae advanced datatype like lists, hashes and sets
> doing data sorting, ranking
> Data Persistence
> Muti AZ
> Pub/Sub capabilities

# ==================================================================
# S3
# object storage
# https://blog.westerndigital.com/why-object-storage/
# https://cloudian.com/blog/object-storage-vs-file-storage/
> secure and scalable object storage that can be access from anywhere
> file can be upto 5TB
> unlimited Storage
> File are stored in a Bucket
> S3 is a universal name space, s.t. name must be unique

# Consistency Model for S3
> Read after Write consistency for PUTS of new file, as soon as file created , 
will be able to read 
> Eventual consistency for overwrite PUTS and DELETS(e.g. change ,delete file)

# object based model
> Key: name of the object
> Value: the data itself , a sequence of bytes
> Version ID
> Metadata:data about the data that you are storing, like owner ,create date,context
> Subresources: bucket specific config
(e.g. bucket polices, access control list, cross origin resource sharing(CORS), make bucket
in another region to get access to the bucket, Transfer acceleration: increase the speed
of transfering files)

# The basics
> build for 99.9% available for the S3 platform.(uptime, the amount of time the service
is available)
> Amazon guranteee 99.9% availability 
> Amazon guarantess 99.99999999999%(11 9 s) durability for S3 inforamtion. Meaning the amount of
data that expected to loose for a given year.
> Tiered Storage Available
> Lifecle Management: set rules to move data amoung different tires.
> Versioning: Version control
> Encryption
> Secure data access using control lists and bucket polices

# s3 Storage Ties/Classes
# durability vs availability
# https://blog.westerndigital.com/data-availability-vs-durability/#:~:targetText=Availability%20refers%20to%20system%20uptime,can%20deliver%20data%20upon%20request.&targetText=Durability%2C%20on%20the%20other%20hand,rot%2C%20degradation%20or%20other%20corruption.
> S3: 99.99% availability and 99.9999999999% durability, stored redundantly across multiple devices in
multiple facilites, and is designed to sustain the loss of 2 facilites concurrently.
> S3 - IA(Infrequent Accessed): For data that is accessed less frequently, but requries fast access
when needed. Lower fee than S3, but are charged on a retrival basis.
> S3 - One Zone: Same as IA however data is stored in a single Availability Zone only, still 99.999999999%
durability, but only 99.5% availability. Cost is 20% less than the regualr S3-IA.
(Since only stored in one availability zone, if that availability zone is down then service will not
be available , but data will still be there once the availability zone is back online)
> Reduced Redudancy Storage: Designed to provided 99.99% durability and 99.99% availability of objects
over a given year. Used for data that can be recreated if lost, e.g.thumbnails
> Glacier: Very cheap, but used for archieve only. Optimised for data that is infrequenctly accessed and it
takes 3-5 hours to restore from Glacier.
> S3 Intelligent Tiering
  - Unknow or unpredictable access patterns
  - 2 tiers - frequent and infrequent access
  - Automatically moves your data to most cost-effective tier based on how frequently you access each
  object
  - 99.9999999999% durability
  - 99.9 availability oaver a given year
  - Optimize cost
  - No fee for accessing your data but a small monthly fee for monitoring/automation $0.0025 per
    1,000 objects

# S3 charges
> Storage per GB
> Requests(Get, Put, Copy,etc)
> Storage Management Pricing(Inventory,Analysis, Object Tags) 
> Data Management Pricing(Data tranfer out of S3)
> Transfer Acceleration(Use the cloud front to optimise transfers)

# S3 FAQ
https://aws.amazon.com/s3/faqs/

# S3 Security 
> By default all newly created buckets are PRIVATE
> Can set up access control to the bucket using:
  - Bucket Policies: apply at the bucket level
  - Access Control List: Apply at object level
> S3 bucket can be configured to create access logs, which log all requests made to the S3 bucket.
  These logs can be written to another bucket. 

# lab
> can the S3 bucket policy bydefault is private 
> can goest to Bucket policy and the policy generator to define the policy for this bucket
  which defines that entiy would have access to the S3(using ARN:amazon recourse number)
  it will then generate the jason for the policy, then we can copy the jason file and paste
  to the Bucket policy editor
> Bucket ACLs allow you to control access at a bucket level, while Object ACLs allow you 
  to control access at the object level.

# S3 encryption
> In Transit: when transfer data from and to S3(SSL/TLS)
> At rest:
  -Server Side Encyrption:
    -S3 Managed Keys -SSE-S3, encrypted each object encrypted with a unique key, use strong multi
     factor encryption , and key are encrypted by the master key will is managed by the amazon
    -AWS Key Management Service, Managed Keys, SSE-KMS:can have log of encryption and decrytion 
    -Server Side Encryption with Customer Providd Keys - SSE-C: aws will do the encryption and
    decryption , but the customer will manage the key.
  
  -Clinet Side Encryption: client encrypt the file before upload

# Enforce encryption on a S3 bucket
> need to initiate PUT request, everytime upload a file 
> There is a file:
  Expect: 100-continue
  Meaning to wait for S3 ack before upload the body of the message
  would not upload the body to S3 if the header is rejected
> if file need to be encrypted at upload time, then two options are Available:
  - x-amz-server-side-encryption: AES256(SSE-S3 - S3 managed keys)
  - x-amz-server-side-encryption: ams:kms(SSE-S3 - S3 managed keys)
  if the param is included in the header of PUT , then S3 will do the encryption accordingly at
  the upload time of the file.
> Serverside encryption can be enforced by using a Bucket Policies which denies any S3 PUT request
which doesn't include the x-amz-server-side encryption.


- summary
> Encyrption in Transit
  - SSL/TLS(HTTPS)
> Encyrption at Rest
  -Server Side Encyrption
    -SSE-S3(Key managed by S3)
    -SSE-KMS(Kye managed by aws)
    -SSE-C(Client managed key)
  -Client side encryption(encryption before upload the file)
> To enforce the use of encryption for files stored in S3, use S3 bucket policy to deny all PUT request
  that does not inlcude the x-amz-server-side-encryption params in the request header.

# Encryptio on S3 lab
> when adding bucket policy , could be error, just add wild card by the end of the recources path

# CORS:corss origin resource shareing , which allows code in one S3 bucket to get access code
# in another S3 bucket
> Create S3 bucket -> property tab-> static hosting website
the end point URL is used for one resources to get acess to another
> Bucekt -> property -> CORS configuration -> add the configuration file
<CORSConfiguration>
    <CORSRule>
        <AllowedOrigin>my_recourse_end_point_url</AllowedOrigin>
        <AllowedMethod>GET</AllowedMethod>
        <MaxAgeSeconds>3000</MaxAgeSeconds>
        <AllowedHeader>Authorization</AllowedHeader>
    </CORSRule>
</CORSConfiguration>

# Cloud Front(Content Deliery Network)
# edge server could cache the content for faster read acess
> Edge Location: the location where the content is cached and can also be written. Separate to an
AWS Region/AZ
> Origin: origin of the files. can be an S3 Bucket, an EC2 Instance, a Elastic Load Balancer or Route53
> Distribution: Name given the CDN, which consists of a collection of Edge Locations.
> Web Distribution: used for Web
> RTMP: used for Median Streaming

> could derliver entire web, including dynamic , static , streaming and interactive content using a
global network of edge locations. Request for the content are automatically routed to the nearest edge
location, so content is delivered with the best performace.
e.g. optimize performance for web access backed by a S3.

# S3 Transfer Acceleration
> instead of user directly upload the file to the S3 bucket, it will uploaded to the region edge server
and then the edge server will proprogate the update to the S3 bucekt.

# Cloud Front lab
network -> Cloud Front -> Create Distribution

> Create Distribution 
  -Restrict Bucket Access: all access to the bucket hass to go through the cloud front
  -Origin Access Identity: to allow clound front the get acess S3 bucekt since, only the
   owner of S3 can get acess to it by default
  -Grant read permission on the Bucket -> Yes,Update Bucket Policy
  -Default TTL: set the time to the fraction of the possible update
  -Restrict Viewer Access: Choose whether you want CloudFront to require users to access your 
   content using a signed URL or a signed cookie(e.g. only reveal content for paid user, client
   will get the URL, and the client can not share the URL since it meant to be for only that client)
  -AWS WAF Web ACL: packet filtering for well known attacks protect at the application layer
  -Alternate Domain Names: can add your own client register domain name to get access cloud front
  (Takes about 10-15 mins ,since it takes time to proprogate the configuration update to all edge servers)

Once the Distribution is created
> Restriction tab: can restrict district to acess the content
> Invalidation tab: remove files in the cache(will be charge a fee)

Use cloud Front to access the S3
> change the S3 permission to not allow public access
> use the cloud front distribution domain name follow by the file path

# S3 Performance Optimisation
if get 100 PUT/LIST/DELET? or > 300 GET request per second
> GET intensive workload: use CloudFront
> Mix Request time: choose random S3 name s.t. object would be store in different partitions 



# ---------------------

# ==================================================================
# Serverless Computing
# Lambda
> upload your code to create a lambda function.You do not need to worry about operating system
patching, scaling, which is managed by amazon lambda. Can use lambda in the two ways:
-Event-driven: change to data in S3 or an Amazon DynamoDB Table

> language support by lambda:
- Node.js
- Java
- Python
- C#
- Go

# How priced
> Number of request:first 1 million request is free. $0.2 per 1 million requests thereafter per month
> Duration: the time your code begins executing until it returns or terminates, round to the nearest
100ms. Depends on the amount of memory you allocate to the function, $0.00001667 per GB.


- Summary
> Scale out
> 1 event = 1 function
> Serveless
> Lambda functions can trigger other lambda functions, 1 event can = X functions if functions
trigggers other functions 
> architecture can be very complicated and hard to debug, need AWS X-ray to debug
> lamda can do things globally, e.g. to back up S3 buckets to other S3 buckets etc.
> Know your triggers


# API gateway
can create API gateway as "front door" for application to access data, business logic, or
functionality from the backend services,e.g. EC2, Lambda, or any web application
> Expose HTTPS endpoints to define a RESTful API
> Serverlessly connect to services like Lambda & DynamoDB
> Send each API endpoint to a different target
> Run efficiently with low cost
> Scale effortlessly
> Track and control usage by API key
> Throttle requests to prevent attacks
> Connect to CloudWatch to log all requests for monitoring


# How to config a API Gateway
> Define an API
> Define resources and nest resources(URL paths)
> For each resources:
  -Select suported HTTP methods(verbs)
  -Set Security
  -Choose target(e.g. EC2, Lambda, DynamoDB, etc)
  -Set request and response transformations
> Deploy API to a a stage
  -Uses API Gateway domain , by default
  -Can use custom domain
  -supports Aws Certificate Manager:free SSL/TLS certs

# API caching
API Gateway can cache your endpoint's response s.t. can reduce the calls
to the endoint and improves the latency of the request to your API.
It cache the response for a specified TLL

# same origin policy
web browser permits scripts contained in a first web page to access data in a 
second web page ,but only if both web pages have the same origin.
To prevent Cross-site Scripting(XSS) attacks.
- Enforced by web browser
- Ignored by tools like Postman and curl

# cross origin recource sharing(CORS)
# https://www.youtube.com/watch?v=Ka8vG5miErk
server at the other end can relax the same origin policy.
allows restricted resources(e.g. fonts) on a web page to be requested from another
domain outside the domain from which the first resources was served
> browser makes an HTTP OPTIONS call for a URL
  -OPTIONS is an HTTP method like GET,PUT and POST
> Server returns a response:
  "These other domains are approved to GET this URL"
> Error
  "Origin policy cannot be read at the remote resource"
   You need to enable CORS on API Gateway.

# serverless web
> for static web hosting the bucket name and domain name has to be the same

lambda -> creae lambda function -> Author from stcratch
> create role
> define the lambda function -> save
-> add triggers -> choose API gateway-> configure trigger -> create an new API
-> add -> API gateway need to be configure(click on the API Gateway)
-> add methond and choose what lambda function to invoke by the method 
-> action -> deploy API
-> stage -> stage name -> API gateway name -> invokation URL
# S3
-> upload index.html and error.html and make them public make the bucket public

# e.g. lambda function
# https://www.youtube.com/watch?v=Ka8vG5miErk
def lambda_handler(event, context):
    print("In lambda handler")
    
    resp = {
        "statusCode": 200,
        # To enable cross-origin resource sharing (CORS) for a proxy integration, 
        # you must add Access-Control-Allow-Origin: <domain_name> to the output headers.
        # <domain_name> can be * for any domain name.
        "headers": {
            "Access-Control-Allow-Origin": "*",
        },
        "body": "Erik Zhou"
    }
    return resp

# version control with lambda
can work with different versions of lambda functions. Each lambda version has unique Amazon Resource Name
After publish a version , it is immutable
$LATEST is the version that can be modified
> Qualified ARN: the function ARN with the version suffix: ...$LATEST
> Unqualified ARN: the function ARN without the version suffix

# Alias
can create an Alias that points to a particular version, and can then subsequently be invoked by calling
the alias.
can promote a particular version to production by simply doing the remapping of the alias

# lab
> when save a lambda function , Qualifier tab -> version (will see the latest version being saved)
when clicked in can see the latest version in the ARN
ARN - arn:aws:lambda:ap-southeast-2:422882508432:function:myNewFunction:$LATEST
-> Action Tab -> publish version(This version can not be changed)
-> Action Tab -> create alias 

> cans split traffic by creating alias relates to additional version ,can specify weight 
for each version, could not use the $LATEST version(e.g. for AB testing)

# Alexa
Amazon Polly -> paste a text into the texbox -> Synthesize to S3
-> S3 Synthesize side bar -> wait till complete the task

lambda -> serverless repo -> alexa-skills-kit-nodejs-factskill-> Deploy
(will depoly alexa skill to lambda)
get back to the lambda function once the alexa skill competed deployed ->
save changes to the lambda function -> get the arn of the lambda function 

https://developer.amazon.com/ -> create a new skill -> give skill a name
-> choose a temple: fact -> side bar invocation: given a invocation name
-> end point sidebar -> paste the lambda arn(point the skill to the lambda function)
-> save endpoints
-> side bar intents -> GetNewFactIntent -> can add new utterance
-> save -> build model
-> once built can goes to the test top bar -> skill test enabled in Development
# link the S3 mp3 file to the alexa 
-> Goes to the S3 bucket -> choose the bucket and copy the object URL
-> goes to the lambda function , change the data to the URL
const data = [ # mark up for audio
  '<audio src = \ "https://alexa-erik.s3-ap-southeast-2.amazonaws.com/3348baca-e737-46b1-8762-e38e08a80f95.mp3\"/>',
];


# step funcions
allows you to visualise and test your serverless applications.provides graphical console to 
arrange and visualised the components of your application as series of steps.
Step functions automatically triggers and tracks each step, and retries when there are errors,
so your application excute in order as expected. Step functions logs the state of each step for debugging.

# step functions lab
aws Services -> Application Integration -> step funcstions
-> state machine -> create state machine -> Run a sample project
-> Job poller -> Deploy(Takes about 10 mins) -> generate a sample lambda function
-> CloudFormation -> select the stepfunctions that create -> action -> delete

-Summary
> visualise your serverless application
> automatically triggers and tracks each step.
> logs the state of each step for debuggging


# X ray
collects data about requests that your application servs and provides tools you can use to view
filter, and gain insight into the data to identify issues and opportunities for optimization.

# X ray SDK
> interceptors to add to your code to trace incomming HTTP requests
> client handlers to instrument aws SDK clients that your applications uses to other aws services
> an HTTP clien to use to instrument calls to other internal and external HTTP web services

# intergration
> Elastic Load Balancing
> AWS Lambda
> API gateway
> EC2
> Elastic Beansstalk

# supported language
Java; Go; Node.js; Python ; Ruby; .Net

# lab
Services -> Elastic Beansstalk -> choose the language
-> Deploy(about 10 mins) -> click on the URL -> Generate traffice
-> xray console
in order to see more services need to give ec2 instance the role that have the permission to access the 
services
-> IAM -> lasticbeanstalk-ec2-role -> attache polies(xray,S3, EC2, DynamoDB)
upload some code to play tictactoe, that interact with S3, DynamoDB
-> Elastic Beansstalk -> choose the application -> upload code(takes about 10 mins) 
once the service map is shown can click on the circle to view more details and can view traces to debug
click on the error and will be able to see the log message of where the error occurs
-> Elastic Beansstalk -> configuration -> software-> modify -> change the email address to the user email
-> apply

-> Elastic Beansstalk -> choose the evn that you have created -> delete 

# Advanced API Gateway
can import an API from an external definition file into API gateway. Currently, the import API feature
supports Swagger v2.0 definition files.

# API Throttling
By default ,API gateway limits the steady-sate request rate to 10,000/sec.
The maximum concurrent requests is 5000 requests across all APIs within an AWS account.
If goes over , will get 429 too many request error

Can configure API gateway as SOAP web service passthrough

# ==================================================================
# DynamoDB
NoSQL database service for all applications that need consistent, single-digit milisecond latency at
any scale. supports both document and key-value data models. flexible data model and reliable performace
suitable for mobile, web, gaming, ad-tech, IOT and many other applications.Good for serverless architecture
> Stored on SSD storage
> spread across 3 geographically distinct data centres
> 2 consitency models:
  -Eventual consistency Read(default, update within a second)
  -Strongly consistent Reads (read the latest update)

> Tables
> Items(row)
> Atrributes(column of data in a table, as key value pairs)
> Supoorts key-value and document data structure
> Document can be written in JSON, HTML or XML

# primary key
> store and retrive data based on primary key
> Partition key: unique atrributes(e.g. userID)
  - value of the partition key is input to an internal hash function that will determine the physical
  location of where the data will be stored
  - if you are using the Partition key as your primary key, then no two items can have the same
  Partition key
> Composite key(Partition key + Sort Key): Same user posting multiple times to a forum
  -Partition key: e.g. user ID (can be same)
  -Sort key: Timestamp of the post(will be different if same partition key)
  -All items with the same Partition Key are stored together, then sorted according to the Sort
  Key value
  -Allows you to store multiple items with the same Partition key

# Access control
> mananged by IAM ,can create a user that have access to DynamoDB
> can create an IAM role enable you to obtain temporary access keys which can be used to
access DynamoDB
> can use a special IAM condition to restrict user access to only their own records
  -e.g. online gaming , user can only access to data relate to themselves, 
  this can be done by adding a Condition to an IAM policy to allow access only to items 
  where the Partition key value matches the user ID.
  for the grained access control using IAM Condition Parameter
  e.g. the dynamodb:LeadingKeys parameter allows user to access only the items where the
  partition key value matches their user ID. 


# Dynamic DB Lab
> create a EC2 instance and run the the apache server when the ec2 start
> download the dynamodb repo
> install the aws skd for php
> install php and php aws skd
> use php script the create DynamoDB tables and populate with datas
> use command line to get access to DynamoDB
# key is the partition key, and N means numerical value with its value given
aws dynamodb get-item --table-name ProductCatalog --region eu-west-2  --key '{"Id":{"N":"205"}}' 

# Index
data structure which allows to perform fast queries on specific columns.
you can select the cloumns that you want included in the index and run search
on the index - rather htan on the entire dataset
- Local secondary index
  > can only be created whne you are creating your table
  > You cannot add, remove , or modify it later
  > same partition key as your original table
  > different sort key
  > different view of your data, organised according to an alternative sort key
  > Any queries based on this sort key are much faster using the index than the main table
  > e.g. Partition key: User ID, sort key : account creation date
- Global secondary index
  > create when create table or add it later
  > different partition key as well as a different sort key
  > gives completly different view of the data
  > speed up any queries relating to this alternative parttion and sort key
  > e.g partition key: email address,sort key:last log-in date

-summary:
- run faster queries on specific data cloumns  
- different view of your data
- difference of the two index


# scan and query API calls
-Query
> Query find items in a table based on the primary key attribute and a distinct value
  to search for
  e.g. given user ID 123 , will select all the attributes for that item
> can use a optional sort key name and value to refined the result
  e.g. if sort key is the time stamp, would be selet emails within the last 7 days
> By default Query returns all the attributes for the item, but can use ProjectionExpression
  parameters to select only the attributes of interets
> Results are always sorted by the sort key in ascending order
> reverse the order by setting the ScanIndexForward parameter to false
> By default,Queries are eventually consistent
> explicitly set the query to be strongly consistent

-Scan
> scan every item in the table, by default returns all data attributes
> use the ProjectionExpression params to refine the scan to only return
  the attributes you want(use filter)
>  dump the whole table and then apply the filter 

- Query or Scan?
Query is more efficient

- Improve Performance
> reduce the impact of a query or scan by setting a smaller page size
  which uses fewer read operations.(e.g. set page size to return 40 pages)
> large number of smaller operations will allow other requests to succeed
  without throttling
> avoid scan, design tables in a way you can use Query,Get, or BacthGetItem APIs. 
> By default , scan operation is sequential, returning 1 MB increments before
  moving on to retrive the next 1MB data.it can only scan one partition at a time
> Can configure DynamoDB to use parallel scans by logically dividing a table or index
  into segments and scanning each segment in parallel.
> Best to avoid parallel scans if your table or index is already read write heavy

-summary
> Query find the items in a table only using the primary key attritbutes
> You provide the primary key name and a distinct value to search for
> A scan operation exame every item in the table
> By default returns all data attributes 
> using the ProjectionExpression params to refine the result

# provisioned throughput
Measured in capacity units
> specify your read and write capacity units when create the table
> 1 write capacity = 1x1 KB write per second
> 1 read capacity = 1 x strongly consistent Read of 4KB persecond
                    2 x eventual consistent Reads of 4KB persecond

# on demand capacity pricing
> charge apply for reading, writing, and storing data
> no need to specify the capacity , DynamoDB will scale up and down accordingly based
on the activity of your application 
> Greate for unpredictable workloads

# DynamoDB Accellorator(DAX)
Clustered in-memory cache for DynamoDB
> up to a 10x read performance improvement
> ideal for Read-Heavy and hursty workloads(auction, gaming,retail during black friday)

# how DAX workds
> is a write through caching service , data is write to the cache and the back end
> when query the DB , will check if in the cache , to speed up read
> if item not in the cache,  DAX will perform an eventual consistent GetItem operation,
  against DynamoDB
> DAX reduce the read load on the DB, maybe able to reduce provisionalread capacity
# not suitable for
> caters for Eventual Consistency read only , so not suitable for application require
  Strongly Consistent reads
> write intense
> do not perform many reads
> do not require micorsecond response time

# Elastic Cache
in memory cahce in the cloud
> sits beteen appliclation and database:
> take the loaf off the database
> Good for read >> write DB

# Types of ElastiCache
- Memcached
> memory object caching system
> Multi-threaded
> No Multi-AZ capability
- Redis
> Open-source in-memory key-value store
> support more complex data structures: sorted sets and lists
> support Master/slave replication and Muli-AZ for cross AZ redundancy

# caching strategies
- lazy loading:
> if data is in the cache read the cache, else, it will fetch the data from
  the DB and store it in the cache for next read
pros:
  > only requested data cached
  > node failutre is okay , just meaning with an empty cache
cons:
  > cache miss penalty
  > Stale data, data is updated only when there is cache miss
  and does not update automatically when the data in the DB changes
TTL:
  keep cache for the specified seconds and then then TTL expires has to goes
  to the DB to request for data, this does not eliminate stale data, but helps
  to avoid it
  
-write through
adds or updates data to the cache whenever data is written to the database
pros:
  > data not stale
  > user generally more tolerant of additional latency when updating data 
  than when retrieving it
cons:
  > write penalty: write to the cache and to the database
  > if a node failed and a new one is up , data is missing until added or
    updated in the database(mitigate by implementing lazy loading in conjunction
    with write-through)
  > waste resources if most of the data never read

DAX only support write through caching

# TTL 
# create a session table
aws dynamodb create-table --table-name SessionData --attribute-definitions \
AttributeName=UserID,AttributeType=N --key-schema \
AttributeName=UserID,KeyType=HASH \
--provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5

# populate the session table
aws dynamodb batch-write-item --request-items file://items.json

DynamoDB -> Table -> select a table -> items -> action -> manage TTL

# DynamoDB Stream
> Time ordered sequence of item level modifications(insert, update, delete)
> logs are encrypted at rest and stored for 24 hours
> accessed using a dedicated endpoint separte from the DB
> By default the primary key is recorded
> can capture the image before and after the change
> usually can be used as event trigger to run lambda function
> good event sousrce for lambda

e.g. when a new invoice is added to the DB , will trigger the lambda to
send notification to the client ,or add an item to the customer's bill

# provisioned thorughput exceeded exception
> if data request rate is too high for the read/write capacity provisioned on
the DB table.
> SDK will automatically retries the requests until successful
> if you are not using the SDK you can:
  -reduce the request frequency
  - use exponential backoff
# ==================================================================
# KMS (key management service)
make it easy for you to create and control the encryption keys and used to encrypt
your data.
> encryption key for KMS are regional
> create a key -> KMS console -> create key
> Customer Master Key(CMK)
  - alias
  - create date
  - Description
  - key date
  - key material(customer provide or AWS provide)
  - can never be exported
> set up a customer master key:
  - create alias and description 
  - choose material option
  - Define key administrative permissions
    - IAM user/roles that can administer(but not use) the key through the KMS API.
  - Define the key usage permissions
    - IAM users/roles that can use the key to encrypt and decrypt data.

# KMS API calls
-> launch an EC2 instance
-> goes to the that would use the encryption key, deactive the existing
   access key and create a new key and save it.
-> aws configure -> get the access key ID and the key that previously saved
-> key id can be found in the KMS session

# encrypt the file using the key we created
aws kms encrypt --key-id 66efd6e9-a8d5-4004-9692-a1bac9f159c0  --plaintext fileb://secret.txt --output text --query CiphertextBlob | base64 --decode > encryptedsecret.txt
# decrypt the file
aws kms decrypt --ciphertext-blob fileb://encryptedsecret.txt --output text --query Plaintext | base64 --decode > decryptedsecret.txt
aws kms re-encrypt --destination-key-id 66efd6e9-a8d5-4004-9692-a1bac9f159c0
 --ciphertext-blob fileb://encryptedsecret.txt | base64 > newencryption.txt
# only key administrator can have the previllege
aws kms enable-key-rotation --key-id 66efd6e9-a8d5-4004-9692-a1bac9f159c0

-summary
> aws kms encrypt
> aws kms decrypt
# re-encrypt cypher text when key has been rotated or reencrypt under different context
> aws kms re-encrypt 
> aws kms enable-key-rotation

#envolop encryption
encrypt the key that will be used to encrypt the data
> customer master key used to decrypt the data key(envelope key)
> Envelop Key is used to decrypt the data


# ==================================================================
# SQS
> distributed message quque
> decouple the components of an application so they run independently,
  easing message manangement between components, e.g. application failed while processing
  client request, would not lose the request, other instance of the application can pick up
  the request not being processed in the que
> Any component of a distributed application can store messages in the queque.Messages can contain
  up to 256 KB of text in any format. Any component can later retrieve the message using Aamazon SQS API
> buffer between the producer and consumer, s.t. when producer burst produce requests , requests can be 
  bufferred for later processing, e.g. could auto scale Ec2 instance based on number of messages in the que
> standard queque:by default,guarantee that message is deliverred at least once. However occationally, more
  than one copy of a message might be deliverred out of order. standard queque provide best effort ordering,
  let you have nearly-unlimited number of transactions per second.
> FIFO Queque:message delivered exactly once, order that message being sent and delivered is perserved.
  also supports message groups that allow multiple ordered message groups within a single queque. FIFO
  queque are limited to 300 transactions per second, but have all the capabilities of standard queques
key facts:
> pull based
> message size 256k
> kept in queque from 1 mins to 14 days, default as 4 days
> guarantees that message will be processed at least once.  
> visibility timeout: 
  - time that message is invisible in he que after a reader picks up that message
  if the message is processed within the timeout , it will be then deleted , else it will become visible
  and other readers might pick it up for processing and would results in the message being delivered 
  multiple times
  - Default timeout is:30s
  - Max:12 hours
> long poll: instead of poll the que and return immediately, long polling would not return until a message
  arrives in the message queque, or the long pull times out. save the frequency of polling.
  
# simple notification service
Amazon simple notification service(SNS):highly scalable flexible , and cost effective capability to publish
message from an application to subsribers or other applications
> push notification to apple, google, windows devices
> can also deliver notification by SMS text message or email to SQS or any http endpoint 
> can also trigger lambda functions, if the lambda function substribed to the the topic, then lambda function
  will be run with the payload of the message as input parameter,function can manipulate the information 
  in the message, publish the message to another SNS topic or other aws services. 
> SNS allows your to group multiple recipients using topics. A topic is an access point for allowing
  recipients to dynamically subscribe for identical copies of the same notification
> Once topic can support deliver to multiple endpoint types(IOS, android...) when publish a topic, SNS will
  deliver he appropriated formated copies of your message to each subscriber..
> messages published to SNS are stored redundantly across multiple availability zones 
> pricing 
  - $0.5 per 1million SNS request 
  - $0.06 per 100,000 notification over http
  - $0.75 per 100 notification over SMS
  - $2 per 100,000 notification over email

# SES vs SNS
SES: email service for sending like marketing notifications or transactional email to the client
using a pay as you go model
> receive emails, incomming mails will be delivered automatically to a S3 bucket
> incomming mails can be used to trigger lambda functions and SNS notifications
> automated emails for purchase confirmation, newsletters, promotions

# Elastic Beanstalk
service for deploying and scaling web applications developed in many languages
onto server platform like Apache, Nginx, Passenger, and IIS
Developers can focus on writing code and do not need to worry about any of
the underlying infrastructure needed to run the application
> upload the code and Elastic Beanstalk will handel depolyment, capacity provisioning,
  load balancing, auto-scaling and application health
> full control of the resources power your application and pay only for the aws resources
  required to store and run you replications,(e.g. EC2 instances and S3 bucekts)
> Managed platform updates feature automatically applies updates your os, Java...
> mannage and monitor your appplication health via dashboard
> integrated with CloudWatch and X-Ray for performance data and metrics

# Elastic Beanstalk Deployment
- All at once
  > Deploy the new version to all instances simultaneously
  > All of the instance will be out of service until deployment takes place 
  > not good for mission critical production system
  > if update fails , need to roll back to the previous version to all the instances
- Rolling depolyment Policy
  > Deploys new version in batches 
  > Each batch of instances is taken out of service while the deployment takes place
  > performance capacity will reduce by the number of instance being deployed
  > not ideal for performance sensitive systems
  > if update fails, you need to perform an additional rolling update to roll back
    the changes
- Rolling with additional Batch policy
  > lauch additional batch of instance for deployment
  > application capacity will not be affected
  > if update fails, you need to perform an additional rolling update to roll back
    the changes
- Immutable deployment policy
  > Deploys the new version to a fresh group of instances in their own new auto scaling
    group
  > when the new instances pass their health checks, they are moved to your existing
    auto scaling group; and finally the old instances are terminated. 
  > application capacity will not be affected
  > the impact of failed update is far less, and the rollback process requires only
    terminating the new auto scaling group.(old version will stay and just need to delete
    the new instance and auto scale group to get back to its previous version)
  > perferred option for Mission Critical production systems.
  
# lab
> application uploaded will be saved in S3 bucekt
application version -> see all versions of the application being uploaded
goes to the applicaton env -> Configuration -> rolling updates and deployments
-> choose the deployment policy

-> application version -> chooose the version to deploy -> action -> deploy
-> env -> create new environment -> create version 1 in one environment and version in another

# Advanced Elastic Beanstalk
> can use Elastic beanstalk configuration files to configure
packages to install, create linux user and groups, run shell commands, specify
services to enable or configure load balancer
> files are written in YAML or JSON format, must end with .config extension and be 
  saved inside a folder called .ebextensions
> .ebextensions must included in the top level directory of your application source
  code bundle, which means that configuration files can be placed under source
  control along with the rest of your application code

# RDS & Elastic Beanstalk
> can launch RDS instance from within the Elastic Beanstalk console, which means
  the RDS instance is created within your Elastic Beanstalk environment, good option
  for test and development.
> This is might not be ideal for production environment, since the life cycle of your
  databse is tied up to the environment, if the environment gets terminated , so will
  the databse instance.
> ideal for productions environment is to launch the RDS outside of Elastic Beanstalk
  This option would gives you a lot more flexibility, allows you to connect multiple
  environments to the same database, provides a wider choice of database types, and 
  allows your to tear down your application environment without affecting the database
  instance.
> access to RDS from Elastic beanstalk: 
  - Additional security group must be added to your environment's auto scaling group
  - provide connections string configuration information to your application servers
    (endpoint , password, using Elastic Beanstalk environment properties).


# Kinesis
> streaming data: data that is generated continously by thousands of data sources, 
which typically send in the data records simultaneouly, and in samll sizes(kilobytes)
e.g. purchase from online store, stock prices
> Kinesis is a platform to send your streaming data, which makes it easy to load and
  analyze streaming data, and also providing the ability for you to build your own
  custome applications for your business needs.

# Kinesis stream
- video streams: securely stream videos from connected devices to AWS for analytics
  and machine leanring
- Data stream: Build custom applications process data in real-time

> stream of data will be send to the Kinesis stream and will be stored in shards for
  default 24 hours , and then the consumer would take the data and process the data
  e.g. EC2 instance
  once the data being processed, then the result will be stored in e.g. S3 or RDS
> shards: 5 transaction per second for read, up to a max total data read rate of 2MB
  per second and up to 1000 records per second for writes, up to a maximum total data
  write rate of 1 MB per second(including partition keys)
> the total capacity of the stream is the total capability of all shards. 

# Kinesis firehose
> no need to specify number of shards, and will scale automatically
> processing the data is optional , but can trigger lambda for processing the data
> data will automatically be stored in S3

# Kinesis Analytics
> run SQL like queries over the stream data over either Kinesis stream or firehose
> results can be stored in S3,Redshift, ElasticCluster

# Kinesis labc
cloud formation -> create stack -> upload the template file
once the stack has been created -> select the stack ->
outputs -> URL: both the data producer and consumer
Kinesis -> streams: will be able to see the stack that we have created

# ==================================================================
# CI & CD:continuous intergration & continuous Delivery/Deployment
> best practice for software development and deployment
> enable frequent software changes to be applied whilist maintaining system
and service stability.
> each team member commit will trigger the build automatically, the built code
  will run agains the test framework(set of automated tests e.g. unit test, intergration
  test, functional test)
> CI focus on small code changes which frquently committed into the main repo once they
  have been successfully tested

  push code to repo -> build -> test -> deploy packaged application -> Environment

Devop White Paper:
https://d0.awsstatic.com/whitepapers/AWS_DevOps.pdf

# aws services
> CodeCommit - source control with git
> CodeBuid - compile source code, run tests and package code
> CodeDeploy - Automated Deployment to EC2, on premisese systems and lambda
> CodePipeline - CI/CD workflow tool, fully automate the entire release process

# CodeCommit lab
CodeCommit -> create repo ->

#CodeDeploy
> In place deployment:(rolling update)
 - system instance will get updated one by one and service capaticty will be reduced
   according to the number of instance being updated 
 - only can be used for EC2 and on premise system, does not suport for lambda
 - if you need to roll back your changes, the previous version of the application
   needs to be redeployed
> Blue Green Deployment:
  - New instances are provisioned and the latest revision is installed on the new instances.
  - onced the new version has been deployed on the new instances , load balancer will redirect
    traffic to the new instances and the old instances will be terminated. 
  - code release is just a matter of switching the load balancer
  - switching back to the orignal environment is faster and more reliable since it only needs
    to switch the load balancer to the old instances(if they are not terminated)
#terminology
> Deployment Group: set of EC2 instances or lambda functions to which a new revision of the 
  software is to be deployed.
> Deployment: The process and the components used to apply a new revision
> Deployment Configuration: a set of deployment rules as well as success/failure conditions
  used during a deployment
> AppSpec File: Defines the deployment actions you want the AWS CodeDeploy to execute
> Revision: Everything needed to deploy the new version: AppSpec file, application files,
  executables, config files
> Application: unique identifier for the applications you want to deploy. To ensure the correct
  combination of revision, deployment configuration and deployment group are referenced during
  a deployment

# lab
create EC2 instance(given a user) -> create an deployment app using the command line 
create a S3 instance(assign a role) -> upload the deployment code to the S3 bucket -> 
CodeDeploy -> the application has been created through command line -> select the app
in application -> create deployment -> now by accessing the EC2 instane the app should be running

# e.g.
user update source code in S3 -> trigger the cloudwatch -> trigger the codePipeLine
-> triger the CodeDeploy -> fetch the updaed code from S3 and deploy to the instances
(e.g. ec2, on premise, etc)

# code pipe line lab

# AppSpec files - lambda Deployments
used to define parameters that will be used for a CodeDeploy deployment. The file structure
depends on whether you are deploying to lambda or EC2/On premises.
For Lambda deployments. the Appsepc file may be written in YAML or JSON and contains the
following fileds:

> version: reserved for furture use - currently the only allowed value is 0.0
> resources: the name and properties of the lambda function to deploy
> hooks: specifies lambda functions to run at set points in the deployment lifecycle to validate
the deployment e.g. validation tests to run before allowing traffic to be sent to your newly 
deployed instances.
  -BeforeAllowTraffic: task to run before traffic is routed to the newly deployed Lamda function
   (e.g. test to validate the function has been deployed correctly).
  -AfterAllowTraffic: used to specify the tasks or functions you want to run after the traffic has been
  routed to the newly deployed lambda function.(e.g. test to validate that the function is accepting
  traffic correctly and behaving as expected).

for EC2 and on Premises:
> version
> OS
> files: location of any application files both(source and destination folder) that needs to be copied
> hook

For EC2 and On premises deployments, the appspec.yml must be placed in the root of the directory of
your revision - the directory containing your application source code.
mywebapp folder:
  appspec.yml
  /Scripts
  /Config
  /Source

# supported hooks for EC2
> BeforeBlockTraffic: run task on instances before they are deregistered from a load balancer
> BlockTraffic: Deregsiter instance from a load balancer
> AfterBlockTraffic: Run tasks on instances after they are deregistered from a load balancer
> ApplicationStop: Gracefully stop the application in preparation for deploying the new revision
> DownloadBundle: the CodeDeploy agent copies the application revision files to a temporary location
> BeforeInstall: Details of any pre-installation scripts, e.g. backing up the current version,
  decrypting files
> Install: the Codedeploy agent copies the application revision files from their temporary location
  to their correct location
> AfterInstall: Details of any post - installation scripts e.g. configuration tasks, change file
  permissions
> ApplicationStart: Restarts any services that were stopped during ApplicationStop
> ValidateService: Details of any tests to validate the service
> BeforeAllowTraffic -> AllowTraffic -> AfterAllowTraffic

# ==================================================================
Docker: A container that is lightweight standalone executable software package which includes
everything the software needs to run code, runtime environment, lib, environment settgins etc.

Aws Elastic Container Service: a fully mananged clusterd platform allows you to run docker images

Aws CodeBuild : build service run sets of commands that you dfine. e.g. compile code ,run test


-summary
> Docker commands to build, tag(alias) and push your Docker image to the ECR repo
  - docker build -t myimagegerepo . # build docker image based on the docker file
  - docker tag muimagerepo:latest URL:latest # create a alias for the image
  - docker push URL # push your docker image into the ECR repo
> use buildspec.yml to define the build commands and settings used by CodeBuild
> can overried the settings in buildspec.yml by adding your own commands in the console
  when launch the build
> if build fails, check the build logs in the CodeBuild console and can also view
  the full CodeBuild log in CloudWatch
  

# ==================================================================
# CloudFormation






# ==================================================================
